{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "T-BKmynFL-BN"
            },
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {
                "id": "PxYugxDxJw2x"
            },
            "outputs": [],
            "source": [
                "!pip install -q unsloth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {
                "id": "FcYBnDtBKC5q"
            },
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "jXWt37LZKODz"
            },
            "outputs": [],
            "source": [
                "max_seq_length = 2048\n",
                "dtype = None\n",
                "load_in_4bit = True"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 350,
                    "referenced_widgets": [
                        "819570f1972e44109056393829dae209",
                        "d2261fa829454905b16396627792b20d",
                        "ac60c5e8aa224d21b22930d48a1ab6f5",
                        "0a4795b587b243e6b35121c51101192b",
                        "3ecf446c6fb34a21a25866d11a1770e8",
                        "938731b9ed6246c283f229fd0b6743d1",
                        "ae37dd673d47436d86cb0f1960152e24",
                        "9e1eb397f6174bc891007ebd2b5cd6a1",
                        "2cbcdd137a6c479c8a35a2fc2192bf7e",
                        "3a3e4dc177b2422a8e37d14617021910",
                        "a6c4bcd2f27b4467bbdd760dd2ea4b4f",
                        "206f1092b7f44f369bb0266a348c9854",
                        "a4ea38de6db04f9c8ed766b7277131fc",
                        "f6eea2ecc0404d469095fb9a3c955ec3",
                        "a0dd25df03ac4ed0888f349db62409b2",
                        "001cfaffbcfa4078bb7508725671a678",
                        "b49b60d3831b4f78a3a5c2c2ebb40be3",
                        "cfddf8a893e5404791c9444e16784e3c",
                        "825cbac8aa2d4495a1fcff0d9cc4ceb7",
                        "9b093bce38f545e8b2b999471b148fd2",
                        "71d2acfa849944ada4db39aa688a58ec",
                        "4316f5bb895345b1925abaa75dda5cd9",
                        "ac6f56692b2b4535a276096c4a344f63",
                        "7d2d9261c1dc494c8c55377fa473c765",
                        "a3dd8c1ea9274de6ac3b2276e8c04730",
                        "768c72ddc4cb4fb0abcf996a7de0dff0",
                        "60351b8ec21842b9a2f3110d842b612e",
                        "70faa8884a204236927857ad6e46f233",
                        "12a4eb3fdeb243e99fdb8c8cd2c0e159",
                        "f83a0356326749d2935d2b745c36163d",
                        "9e9dcc38d1134c89813c05876fcd40c9",
                        "d6e5d9d0540a44f795b9b80b04f2de40",
                        "6c6bbeee132e42a7a3d9038a4571930b",
                        "daac5b80ea654d418cfbffe52ee16fbc",
                        "37c463b5aa23498a8ed53c5e4d5198f7",
                        "8f959946359a46aca889d618d9718fc8",
                        "e8368130e5174ff0a22884c8fedc9448",
                        "1a9d07fb2bc144f1bab0f14ce6ee672a",
                        "4db50af69d75452c836705b2d8bc5a88",
                        "b4d191b3e03848e6a4073ddaf0477af6",
                        "406c4f8f7a8b4586ab1c328ca099c82a",
                        "3c8cb2a28b62414bba353e1a38dbc29d",
                        "b8ab779acd1e466da7db995e4d41c81b",
                        "e7947fa4f58f4ffe8195d62a6b399bd1",
                        "8e9654d2b9f34a4d8782054c138c54a4",
                        "cc8dfb8f830a451f8fe63e9c6b6c48a8",
                        "3beebd78e0ab42408ca84ff18b903a27",
                        "fa0d8ff6c77f408bbb309171f9e2019c",
                        "49f45f79d47f45d1a38cdffbf98f3fe5",
                        "7868e8ddf6b4432184c2974acdcd6916",
                        "1bc5288f94bc467887e44db195c2abe6",
                        "8c6447ecad304abca343aea70c302f7a",
                        "d983dec51f6f4cdb9213c4f7775c32a6",
                        "b4f968909ab84801963182807d77f6f7",
                        "99cbf07f81e24dc1b26bbc4d4e942e83",
                        "23d3cbc8e5b6460590f5d8e900f6d70a",
                        "24a9faf6c02b435c8eb39adc8c508634",
                        "115d205afc824398aa33b234b5575447",
                        "edcaab83e54941439d88c318e5476df7",
                        "b786f9be2ffa4f8782eedaf22a4de5ff",
                        "8aff9ee4144344e4a4f4981eba26adeb",
                        "82d0c9613ced4337b1e8d321e28fa4bb",
                        "d4f76618b0c54e398a55a5c4815b7ced",
                        "29c174a2f9674975bdc8147a7088a17f",
                        "422aa6d8e51442c8aaa0104490668b95",
                        "151c58535f5641eb80406c2172d5afa3"
                    ]
                },
                "id": "_iFXLwHXKN9z",
                "outputId": "8563e201-a985-4914-b029-7aa603ab01de"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "==((====))==  Unsloth 2025.11.4: Fast Llama patching. Transformers: 4.57.2.\n",
                        "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
                        "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
                        "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
                        " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
                        "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "<string>:45: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n"
                    ]
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "819570f1972e44109056393829dae209",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "206f1092b7f44f369bb0266a348c9854",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "ac6f56692b2b4535a276096c4a344f63",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer_config.json: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "daac5b80ea654d418cfbffe52ee16fbc",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "8e9654d2b9f34a4d8782054c138c54a4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "23d3cbc8e5b6460590f5d8e900f6d70a",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "chat_template.jinja: 0.00B [00:00, ?B/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name='unsloth/Llama-3.2-3B-Instruct',\n",
                "    max_seq_length=max_seq_length,\n",
                "    dtype=dtype,\n",
                "    load_in_4bit=load_in_4bit\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "rRR09lfUKN7e",
                "outputId": "dd8bdf8d-08c0-4949-aff3-ec9dac694e6a"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Unsloth 2025.11.4 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
                    ]
                }
            ],
            "source": [
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r = 16,\n",
                "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\", ],\n",
                "    lora_alpha = 16,\n",
                "    lora_dropout = 0,\n",
                "    bias = \"none\",\n",
                "    use_gradient_checkpointing = \"unsloth\",\n",
                "    random_state = 3407,\n",
                "    use_rslora = False,\n",
                "    loftq_config = None,\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {
                "id": "xcUTaLORKN5S"
            },
            "outputs": [],
            "source": [
                "from datasets import load_dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {
                "id": "gk96reHnKN23"
            },
            "outputs": [],
            "source": [
                "dataset = load_dataset(\"ServiceNow-AI/R1-Distill-SFT\", 'v0', split=\"train\")  # HuggingFace lib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "rFB3ADj9KN0k",
                "outputId": "e2bcd951-7cc0-42a3-ac40-b5a4b3930b7d"
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "{'id': ['id_0', 'id_1'],\n",
                            " 'reannotated_assistant_content': ['<think>\\nFirst, I need to determine the total number of children on the playground by adding the number of boys and girls.\\n\\nThere are 27 boys and 35 girls.\\n\\nAdding these together: 27 boys + 35 girls = 62 children.\\n\\nTherefore, the total number of children on the playground is 62.\\n</think>\\n\\nTo find the total number of children on the playground, we simply add the number of boys and girls together.\\n\\n\\\\[\\n\\\\text{Total children} = \\\\text{Number of boys} + \\\\text{Number of girls}\\n\\\\]\\n\\nPlugging in the given values:\\n\\n\\\\[\\n\\\\text{Total children} = 27 \\\\text{ boys} + 35 \\\\text{ girls} = 62 \\\\text{ children}\\n\\\\]\\n\\n**Final Answer:**\\n\\n\\\\[\\n\\\\boxed{62}\\n\\\\]',\n",
                            "  '<think>\\nFirst, I need to determine the cost per dozen oranges. John bought three dozen oranges for \\\\$28.80, so I can find the cost per dozen by dividing the total cost by the number of dozens.\\n\\nNext, with the cost per dozen known, I can calculate the cost for five dozen oranges by multiplying the cost per dozen by five.\\n\\nFinally, I will present the final answer clearly.\\n</think>\\n\\n**Solution:**\\n\\nTo determine the cost of five dozen oranges at the same rate, follow these steps:\\n\\n1. **Find the cost per dozen:**\\n\\n   John purchased three dozen oranges for \\\\$28.80. To find the cost per dozen, divide the total cost by the number of dozens.\\n\\n   \\\\[\\n   \\\\text{Cost per dozen} = \\\\frac{\\\\$28.80}{3} = \\\\$9.60 \\\\text{ per dozen}\\n   \\\\]\\n\\n2. **Calculate the cost for five dozen:**\\n\\n   Now, multiply the cost per dozen by the number of dozens needed.\\n\\n   \\\\[\\n   \\\\text{Cost for five dozen} = 5 \\\\times \\\\$9.60 = \\\\$48.00\\n   \\\\]\\n\\n3. **Final Answer:**\\n\\n   \\\\[\\n   \\\\boxed{\\\\$48}\\n   \\\\]'],\n",
                            " 'problem': ['There were 27 boys and 35 girls on the playground at recess. There were _____ children on the playground at recess.',\n",
                            "  'John purchased three dozen oranges for $\\\\$$28.80. At the same rate, how much would five dozen of these oranges cost?'],\n",
                            " 'source': ['orca_math', 'synthetic_math'],\n",
                            " 'solution': ['\\nThere were 62 children on the playground at recess. (27 boys + 35 girls = $\\\\boxed{62}$  children)',\n",
                            "  'The problem states that John bought three dozen oranges for $\\\\$$28.80. To find the cost per dozen, we use the formula:\\n$$ \\\\text{Cost per dozen} = \\\\frac{\\\\text{Total cost}}{\\\\text{Number of dozens}} = \\\\frac{\\\\$28.80}{3} = \\\\$9.60 \\\\text{ per dozen}. $$\\n\\nTo determine the cost for five dozen oranges:\\n$$ \\\\text{Cost for five dozen} = 5 \\\\times \\\\text{Cost per dozen} = 5 \\\\times \\\\$9.60 = \\\\$48. $$\\n\\nThus, the cost for five dozen oranges is $\\\\boxed{\\\\$48}$.'],\n",
                            " 'verified': [None, None],\n",
                            " 'quality_metrics': [None, None]}"
                        ]
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "dataset[:2]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 49,
                    "referenced_widgets": [
                        "e78e6cddc5954e748f49e7a824b6d7de",
                        "9202f803a935466a95a9747bf4723e5c",
                        "4c1e6d4ab5074122a89d546bc87159cf",
                        "2292fd705ea04472b2248b124e807863",
                        "01b4502f12524bf5a9e90341f6d2e740",
                        "270a53784547479e8e6bddc897eaf5f1",
                        "29b337ccaae2479a9196b426c222a00b",
                        "31897bc4c1da47318300189266cc3d08",
                        "7f221ee5d22b459b8a5dbb7b15c32ab5",
                        "382cca8fe6d245e58289c1164dd0dd44",
                        "72f582db86d74a82bf90cf2222cefe92"
                    ]
                },
                "id": "DwUBJ2bsKNpp",
                "outputId": "740de4e5-89d4-4824-eea1-f885bea89b61"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "e78e6cddc5954e748f49e7a824b6d7de",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Map:   0%|          | 0/171647 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "r1_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking, Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
                "<problem>\n",
                "{}\n",
                "</problem>\n",
                "\n",
                "{}\n",
                "{}\n",
                "\"\"\"\n",
                "EOS_TOKEN = tokenizer.eos_token\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "  problems = examples[\"problem\"]\n",
                "  thoughts = examples[\"reannotated_assistant_content\"]\n",
                "  solutions = examples[\"solution\"]\n",
                "  texts = []\n",
                "\n",
                "  for problem, thought, solution in zip(problems, thoughts, solutions):\n",
                "    text = r1_prompt.format(problem, thought, solution)+EOS_TOKEN\n",
                "    texts.append(text)\n",
                "\n",
                "  return {\"text\": texts}\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched = True,)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {
                "id": "-9O45Et1KNdM"
            },
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
                "from unsloth import is_bfloat16_supported"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 49,
                    "referenced_widgets": [
                        "b74af112d82743f0bce6f8550dd63ef4",
                        "434ec5d1f4c346caaee85f5d755faf66",
                        "520a31fc0b9641a3aad08666cb27bb31",
                        "355b63a40b8945d5a93bf8200d5dd0e9",
                        "8fcab4702dae401a9474a4977de59790",
                        "ca79eaf283b74abd8a522d3f3fcdf5a7",
                        "3174e283951e4166993ed56d3b5a561b",
                        "81fb500c121748f48f37516aba70fd87",
                        "6478b0915a804c44a76abeb0dd0b3fd9",
                        "48cfbd244d224938a32c75cc4e19eca3",
                        "4ee77131c4ef4f1395e1cf95968c447a"
                    ]
                },
                "id": "hrHb3uaUMvoc",
                "outputId": "610ae3ff-cd98-4961-bff6-3a85de4ae885"
            },
            "outputs": [
                {
                    "data": {
                        "application/vnd.jupyter.widget-view+json": {
                            "model_id": "b74af112d82743f0bce6f8550dd63ef4",
                            "version_major": 2,
                            "version_minor": 0
                        },
                        "text/plain": [
                            "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/171647 [00:00<?, ? examples/s]"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                }
            ],
            "source": [
                "trainer = SFTTrainer(\n",
                "    model = model,\n",
                "    tokenizer = tokenizer,\n",
                "    train_dataset = dataset,\n",
                "    dataset_text_field = \"text\",\n",
                "    max_seq_length = max_seq_length,\n",
                "    dataset_num_proc = 2,\n",
                "    packing = False,\n",
                "    args = TrainingArguments(\n",
                "        per_device_train_batch_size = 2,\n",
                "        gradient_accumulation_steps = 4,\n",
                "        warmup_steps = 5,\n",
                "        max_steps = 60,\n",
                "        learning_rate = 2e-4,\n",
                "        fp16 = not is_bfloat16_supported(),\n",
                "        bf16 = is_bfloat16_supported(),\n",
                "        logging_steps = 1,\n",
                "        optim = \"adamw_8bit\",\n",
                "        weight_decay = 0.01,\n",
                "        lr_scheduler_type = \"linear\",\n",
                "        seed = 3407,\n",
                "        output_dir = \"outputs\",\n",
                "        report_to = \"none\",\n",
                "        ),\n",
                "    )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/",
                    "height": 1000
                },
                "id": "AECJ9aFlTQHj",
                "outputId": "75043782-8cb0-4309-817e-3237bd5c8298"
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
                        "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
                        "   \\\\   /|    Num examples = 171,647 | Num Epochs = 1 | Total steps = 60\n",
                        "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
                        "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
                        " \"-____-\"     Trainable parameters = 24,313,856 of 3,237,063,680 (0.75% trained)\n",
                        "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
                        "  return fn(*args, **kwargs)\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Unsloth: Will smartly offload gradients to save VRAM!\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
                        "  return fn(*args, **kwargs)\n",
                        "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
                        "  return fn(*args, **kwargs)\n"
                    ]
                },
                {
                    "data": {
                        "text/html": [
                            "\n",
                            "    <div>\n",
                            "      \n",
                            "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
                            "      [60/60 09:58, Epoch 0/1]\n",
                            "    </div>\n",
                            "    <table border=\"1\" class=\"dataframe\">\n",
                            "  <thead>\n",
                            " <tr style=\"text-align: left;\">\n",
                            "      <th>Step</th>\n",
                            "      <th>Training Loss</th>\n",
                            "    </tr>\n",
                            "  </thead>\n",
                            "  <tbody>\n",
                            "    <tr>\n",
                            "      <td>1</td>\n",
                            "      <td>1.022000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>2</td>\n",
                            "      <td>0.947000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>3</td>\n",
                            "      <td>1.046800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>4</td>\n",
                            "      <td>0.956800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>5</td>\n",
                            "      <td>0.798500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>6</td>\n",
                            "      <td>0.867000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>7</td>\n",
                            "      <td>0.769600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>8</td>\n",
                            "      <td>0.753800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>9</td>\n",
                            "      <td>0.798000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>10</td>\n",
                            "      <td>0.746100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>11</td>\n",
                            "      <td>0.639600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>12</td>\n",
                            "      <td>0.644500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>13</td>\n",
                            "      <td>0.554200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>14</td>\n",
                            "      <td>0.637400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>15</td>\n",
                            "      <td>0.620500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>16</td>\n",
                            "      <td>0.567400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>17</td>\n",
                            "      <td>0.632900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>18</td>\n",
                            "      <td>0.574800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>19</td>\n",
                            "      <td>0.557900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>20</td>\n",
                            "      <td>0.522700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>21</td>\n",
                            "      <td>0.562400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>22</td>\n",
                            "      <td>0.558800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>23</td>\n",
                            "      <td>0.720600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>24</td>\n",
                            "      <td>0.625500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>25</td>\n",
                            "      <td>0.601600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>26</td>\n",
                            "      <td>0.607400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>27</td>\n",
                            "      <td>0.539600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>28</td>\n",
                            "      <td>0.493200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>29</td>\n",
                            "      <td>0.588600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>30</td>\n",
                            "      <td>0.472300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>31</td>\n",
                            "      <td>0.652800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>32</td>\n",
                            "      <td>0.601400</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>33</td>\n",
                            "      <td>0.625500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>34</td>\n",
                            "      <td>0.711300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>35</td>\n",
                            "      <td>0.531100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>36</td>\n",
                            "      <td>0.463500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>37</td>\n",
                            "      <td>0.586100</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>38</td>\n",
                            "      <td>0.595000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>39</td>\n",
                            "      <td>0.585000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>40</td>\n",
                            "      <td>0.601200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>41</td>\n",
                            "      <td>0.606900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>42</td>\n",
                            "      <td>0.591900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>43</td>\n",
                            "      <td>0.592000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>44</td>\n",
                            "      <td>0.557600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>45</td>\n",
                            "      <td>0.484900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>46</td>\n",
                            "      <td>0.552800</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>47</td>\n",
                            "      <td>0.549600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>48</td>\n",
                            "      <td>0.596700</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>49</td>\n",
                            "      <td>0.539500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>50</td>\n",
                            "      <td>0.494000</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>51</td>\n",
                            "      <td>0.534900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>52</td>\n",
                            "      <td>0.504900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>53</td>\n",
                            "      <td>0.624300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>54</td>\n",
                            "      <td>0.579900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>55</td>\n",
                            "      <td>0.702500</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>56</td>\n",
                            "      <td>0.620900</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>57</td>\n",
                            "      <td>0.430600</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>58</td>\n",
                            "      <td>0.693200</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>59</td>\n",
                            "      <td>0.618300</td>\n",
                            "    </tr>\n",
                            "    <tr>\n",
                            "      <td>60</td>\n",
                            "      <td>0.504300</td>\n",
                            "    </tr>\n",
                            "  </tbody>\n",
                            "</table><p>"
                        ],
                        "text/plain": [
                            "<IPython.core.display.HTML object>"
                        ]
                    },
                    "metadata": {},
                    "output_type": "display_data"
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
                        "  return fn(*args, **kwargs)\n",
                        "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
                        "  return fn(*args, **kwargs)\n"
                    ]
                }
            ],
            "source": [
                "trainer_stats = trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {
                "id": "AOTUyRKGUFML"
            },
            "outputs": [],
            "source": [
                "from unsloth.chat_templates import get_chat_template\n",
                "sys_prompt = \"\"\"You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking, Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
                "<problem>\n",
                "{}\n",
                "</problem>\n",
                "\"\"\"\n",
                "\n",
                "message = sys_prompt.format(\"How many 'r's are present in 'strawberry'?\")\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template = \"llama-3.1\",\n",
                ")\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "messages = [\n",
                "    {\"role\": \"user\", \"content\": message},\n",
                "]\n",
                "\n",
                "inputs = tokenizer.apply_chat_template(\n",
                "    messages,\n",
                "    tokenize = True,\n",
                "    add_generation_prompt = True,\n",
                "    return_tensors = \"pt\",\n",
                ").to(\"cuda\")\n",
                "\n",
                "outputs = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True, temperature = 1.5, min_p = 0.1)\n",
                "response = tokenizer.batch_decode(outputs)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "colab": {
                    "base_uri": "https://localhost:8080/"
                },
                "id": "LHcbauYDVrXg",
                "outputId": "c81d8e18-1f6c-4006-b82b-f5f46fb8c6cf"
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                        "\n",
                        "Cutting Knowledge Date: December 2023\n",
                        "Today Date: 26 July 2024\n",
                        "\n",
                        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
                        "\n",
                        "You are a reflective assistant engaging in thorough, iterative reasoning, mimicking human stream-of-consciousness thinking, Your approach emphasizes exploration, self-doubt, and continuous refinement before coming up with an answer.\n",
                        "<problem>\n",
                        "How many 'r's are present in'strawberry'?\n",
                        "</problem>\n",
                        "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
                        "\n",
                        "I think I need to count the number of 'r's in the word \"strawberry.\" Okay, let me write it down: s-t-r-a-w-b-e-r-r-y. Hmm, strawberries are a type of fruit, and they are very popular, especially during summer.\n",
                        "\n",
                        "Alright, so I need to find how many 'r's are in \"strawberry.\" Let me start by breaking down the word into individual letters: s-t-r-a-w-b-e-r-r-y. Each letter should be counted carefully, as the question is looking for the total number of 'r's.\n",
                        "\n",
                        "First, I can see the first \"r\" right after \"s\" in \"s-t-r.\" So that's one 'r.' Then I go to the \"w\" â€“ that doesn't have any 'r's, so moving on.\n",
                        "\n",
                        "Next, I see \"a-r-a\" in that order in \"s-t-r-a-w-b-e-r-r-y.\" That has two 'r's: one in \"r-a\" and another in \"w-b-e-r-r-y.\"\n",
                        "\n",
                        "I know \"w-b-e-r\" is present in the word, but I think I should be looking specifically at the letter combination \"r\". Since the word is \"strawberry\" and I've already counted \"a-r-a\" with two 'r's, the next \"r\" I can find is in the word \"berry\". \n",
                        "\n",
                        "Wait, in \"strawberry\", the letter 'w' in \"berry\" has an 'r', and the last 'r' is right at the end. So, adding this last 'r', the total number of 'r's would be: the one after \"s\", two in \"r-a\", and the 'r' in \"berry\" and the last one.\n",
                        "\n",
                        "Let me recap: s-t-r with an 'r', followed by a second 'r' in \"r-a\" twice, then \"w-b-e\" without 'r', \"berry\" with an 'r', and finally, the last \"r\" stands alone.\n",
                        "\n",
                        "So, adding these up: one for the 'r' after \"s\", two for the \"r-a\", none in \"w-b-e\", one for \"r\" in \"berry\", and one more for the final 'r'. So, 1 + 2 + 0 + 1 + 1.\n",
                        "\n",
                        "If I add those together, that's 1 + 2 + 0 + 1 + 1 = 5.\n",
                        "\n",
                        "Alright, I think that's the correct count. I made sure not to miscount the 'r's in \"strawberry\". If I were to recount, it seems like there are five 'r's: the first after's', then the two in \"r-a\" (since there are two), then there's one 'r' in \"berry\", and lastly, the solitary 'r' at the end.\n",
                        "\n",
                        "So, 5 seems the right number.<|eot_id|>\n"
                    ]
                }
            ],
            "source": [
                "print(response[0])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "I3_P3pAVcE5h"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}
